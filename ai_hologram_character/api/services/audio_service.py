"""
éŸ³é¢‘æœåŠ¡æ¨¡å—
å¤„ç†è¯­éŸ³æ´»åŠ¨æ£€æµ‹ã€è¯´è¯äººè¯†åˆ«å’Œè¯­éŸ³åˆæˆç­‰åŠŸèƒ½
"""
import numpy as np
import time
import threading
import queue
import os
from collections import deque
from typing import Any, Callable, Dict, List, Optional, Tuple

import pyaudio
from TTS.api import TTS
try:
    import librosa
except ImportError:
    print("Warning: librosa not installed, some audio features may be limited")


class AudioService:
    """éŸ³é¢‘æœåŠ¡ç±»ï¼Œå¤„ç†éŸ³é¢‘è¾“å…¥è¾“å‡ºå’Œè¯­éŸ³å¤„ç†"""

    def __init__(self, tts_model_name="tts_models/zh-CN/baker/tacotron2-DDC-GST"):
        """åˆå§‹åŒ–éŸ³é¢‘æœåŠ¡

        Args:
            tts_model_name (str): TTSæ¨¡å‹åç§°ï¼Œé»˜è®¤ä½¿ç”¨ä¸­æ–‡æ¨¡å‹
        """
        # åˆå§‹åŒ–TTSå¼•æ“
        try:
            # ä½¿ç”¨ğŸ¸TTSæ›¿ä»£pyttsx3
            self.tts_engine = TTS(model_name=tts_model_name, progress_bar=False)
            print(f"TTSå¼•æ“åˆå§‹åŒ–æˆåŠŸï¼Œä½¿ç”¨æ¨¡å‹: {tts_model_name}")
        except Exception as e:
            print(f"TTSå¼•æ“åˆå§‹åŒ–å¤±è´¥: {e}ï¼Œå°è¯•ä½¿ç”¨é»˜è®¤æ¨¡å‹")
            try:
                # å°è¯•ä½¿ç”¨é»˜è®¤æ¨¡å‹
                self.tts_engine = TTS(progress_bar=False)
                print("TTSå¼•æ“åˆå§‹åŒ–æˆåŠŸï¼Œä½¿ç”¨é»˜è®¤æ¨¡å‹")
            except Exception as e:
                print(f"TTSå¼•æ“åˆå§‹åŒ–å¤±è´¥: {e}")
                raise
        
        # åˆå§‹åŒ–è¯­éŸ³æ´»åŠ¨æ£€æµ‹ç»„ä»¶
        self.vad = SimpleVAD()
        
        # åˆå§‹åŒ–è¯´è¯äººè¯†åˆ«ç»„ä»¶
        self.speaker_recognition = SpeakerRecognition()
        
        # çŠ¶æ€æ§åˆ¶
        self.is_listening = False
        self.conversation_active = False
        
        # éŸ³é¢‘è¾“å‡ºé…ç½®
        self.output_path = "temp_audio.wav"
        
    def start_listening(self) -> None:
        """å¼€å§‹ç›‘å¬éŸ³é¢‘è¾“å…¥"""
        if not self.is_listening:
            self.is_listening = True
            self.vad.start_listening()
            
            # å¯åŠ¨VADå¤„ç†çº¿ç¨‹
            vad_thread = threading.Thread(target=self.vad.process_audio)
            vad_thread.daemon = True
            vad_thread.start()
            
            print("éŸ³é¢‘æœåŠ¡: å¼€å§‹ç›‘å¬")
    
    def stop_listening(self) -> None:
        """åœæ­¢ç›‘å¬éŸ³é¢‘è¾“å…¥"""
        if self.is_listening:
            self.is_listening = False
            self.vad.stop_listening()
            print("éŸ³é¢‘æœåŠ¡: åœæ­¢ç›‘å¬")
    
    def process_speech_with_pauses(self, audio_data: np.ndarray) -> str:
        """å¤„ç†å¸¦æœ‰åœé¡¿çš„è¯­éŸ³

        Args:
            audio_data (np.ndarray): éŸ³é¢‘æ•°æ®

        Returns:
            str: å¤„ç†åçš„æ–‡æœ¬
        """
        # 1. æ£€æµ‹è¯­éŸ³ä¸­çš„åœé¡¿
        segments, pause_lengths = self._detect_pauses_in_audio(audio_data)
        
        # 2. å¯¹æ¯ä¸ªè¯­éŸ³æ®µè¿›è¡Œè¯†åˆ«
        recognized_segments = []
        for segment in segments:
            text = self._speech_to_text(segment)
            recognized_segments.append(text)
        
        # 3. æ ¹æ®åœé¡¿é•¿åº¦å†³å®šå¦‚ä½•è¿æ¥è¿™äº›æ®µè½
        final_text = ""
        for i, text in enumerate(recognized_segments):
            final_text += text
            
            # æ ¹æ®ä¸åŒé•¿åº¦çš„åœé¡¿æ·»åŠ ä¸åŒçš„è¿æ¥ç¬¦å·
            if i < len(pause_lengths):
                if pause_lengths[i] > 1.0:  # é•¿åœé¡¿
                    final_text += "ã€‚ "  # æ·»åŠ å¥å·å’Œç©ºæ ¼
                elif pause_lengths[i] > 0.5:  # ä¸­ç­‰åœé¡¿
                    final_text += "ï¼Œ "  # æ·»åŠ é€—å·å’Œç©ºæ ¼
                else:  # çŸ­åœé¡¿æˆ–æ— åœé¡¿
                    final_text += " "  # åªæ·»åŠ ç©ºæ ¼
        
        return final_text
    
    def _detect_pauses_in_audio(self, audio_data: np.ndarray) -> Tuple[List[np.ndarray], List[float]]:
        """æ£€æµ‹éŸ³é¢‘ä¸­çš„åœé¡¿

        Args:
            audio_data (np.ndarray): éŸ³é¢‘æ•°æ®

        Returns:
            Tuple[List[np.ndarray], List[float]]: åˆ†å‰²åçš„éŸ³é¢‘æ®µåˆ—è¡¨å’Œæ¯ä¸ªæ®µä¹‹ååœé¡¿çš„æ—¶é•¿åˆ—è¡¨
        """
        # è®¡ç®—çŸ­æ—¶èƒ½é‡
        frame_length = 512
        hop_length = 256
        frames = [audio_data[i:i+frame_length] for i in range(0, len(audio_data), hop_length) if i+frame_length <= len(audio_data)]
        energy = np.array([np.sum(frame**2) for frame in frames])
        
        # è®¾ç½®èƒ½é‡é˜ˆå€¼æ¥æ£€æµ‹åœé¡¿
        threshold = np.mean(energy) * 0.1
        
        # æ ‡è®°ä½èƒ½é‡å¸§(å¯èƒ½æ˜¯åœé¡¿)
        is_silence = energy < threshold
        
        # æ‰¾å‡ºè¿ç»­çš„é™éŸ³æ®µ
        silence_starts = []
        silence_ends = []
        
        if is_silence[0]:
            silence_starts.append(0)
            
        for i in range(1, len(is_silence)):
            if is_silence[i] and not is_silence[i-1]:
                silence_starts.append(i)
            elif not is_silence[i] and is_silence[i-1]:
                silence_ends.append(i)
                
        if is_silence[-1]:
            silence_ends.append(len(is_silence))
            
        # ç¡®ä¿é•¿åº¦åŒ¹é…
        min_len = min(len(silence_starts), len(silence_ends))
        silence_starts = silence_starts[:min_len]
        silence_ends = silence_ends[:min_len]
        
        # è®¡ç®—åœé¡¿æ—¶é•¿(ç§’)
        sample_rate = 16000  # å‡è®¾é‡‡æ ·ç‡ä¸º16kHz
        pause_lengths = [(silence_ends[i] - silence_starts[i]) * hop_length / sample_rate for i in range(len(silence_starts))]
        
        # æ„å»ºéé™éŸ³æ®µ
        speech_segments = []
        segment_start = 0
        
        for silence_start in silence_starts:
            frame_index = silence_start * hop_length
            if frame_index > segment_start:
                speech_segments.append(audio_data[segment_start:frame_index])
            segment_start = silence_ends[silence_starts.index(silence_start)] * hop_length
            
        # æ·»åŠ æœ€åä¸€æ®µ
        if segment_start < len(audio_data):
            speech_segments.append(audio_data[segment_start:])
            
        return speech_segments, pause_lengths
    
    def _speech_to_text(self, audio_segment: np.ndarray) -> str:
        """å°†éŸ³é¢‘æ®µè½¬æ¢ä¸ºæ–‡æœ¬

        Args:
            audio_segment (np.ndarray): éŸ³é¢‘æ®µæ•°æ®

        Returns:
            str: è¯†åˆ«çš„æ–‡æœ¬
        """
        # TODO: å®ç°çœŸå®çš„è¯­éŸ³è¯†åˆ«
        # å®é™…åº”ç”¨ä¸­ï¼Œè¿™é‡Œåº”è°ƒç”¨ASRç³»ç»Ÿ
        return "è¿™æ˜¯ä¸€ä¸ªç¤ºä¾‹è¯†åˆ«ç»“æœ"
    
    def identify_speaker(self, audio_data: np.ndarray) -> str:
        """è¯†åˆ«è¯´è¯äºº

        Args:
            audio_data (np.ndarray): éŸ³é¢‘æ•°æ®

        Returns:
            str: è¯´è¯äººID
        """
        return self.speaker_recognition.identify_speaker(audio_data)
    
    def update_speaker_model(self, speaker_id: str, audio_data: np.ndarray) -> None:
        """æ›´æ–°è¯´è¯äººæ¨¡å‹

        Args:
            speaker_id (str): è¯´è¯äººID
            audio_data (np.ndarray): éŸ³é¢‘æ•°æ®
        """
        self.speaker_recognition.update_speaker_model(speaker_id, audio_data)
    
    def speak(self, text: str) -> None:
        """ç”Ÿæˆè¯­éŸ³è¾“å‡º

        Args:
            text (str): è¦è½¬æ¢ä¸ºè¯­éŸ³çš„æ–‡æœ¬
        """
        try:
            # ä½¿ç”¨ğŸ¸TTSç”ŸæˆéŸ³é¢‘
            wav = self.tts_engine.tts(text=text, speaker=None, language=None)
            
            # ğŸ¸TTSå·²ç»å¤„ç†äº†éŸ³é¢‘æ’­æ”¾ï¼Œä¸éœ€è¦é¢å¤–çš„æ’­æ”¾ä»£ç 
            # å¦‚æœéœ€è¦è‡ªå®šä¹‰æ’­æ”¾ï¼Œå¯ä»¥ä¿å­˜å¹¶æ’­æ”¾
            import soundfile as sf
            import sounddevice as sd
            
            # ä¿å­˜éŸ³é¢‘åˆ°ä¸´æ—¶æ–‡ä»¶
            sf.write(self.output_path, wav, self.tts_engine.synthesizer.output_sample_rate)
            
            # æ’­æ”¾éŸ³é¢‘
            data, fs = sf.read(self.output_path)
            sd.play(data, fs)
            sd.wait()  # ç­‰å¾…éŸ³é¢‘æ’­æ”¾å®Œæˆ
            
            # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
            try:
                os.remove(self.output_path)
            except:
                pass
                
        except Exception as e:
            print(f"è¯­éŸ³åˆæˆå¤±è´¥: {e}")
    
    def speak_with_pauses(self, text: str, emotional_state: Optional[str] = None) -> None:
        """å¤„ç†å¸¦åœé¡¿çš„è¯­éŸ³åˆæˆ

        Args:
            text (str): è¦è½¬æ¢ä¸ºè¯­éŸ³çš„æ–‡æœ¬
            emotional_state (str, optional): æƒ…æ„ŸçŠ¶æ€ï¼Œå½±å“åœé¡¿æ—¶é•¿
        """
        # æ‹†åˆ†æ–‡æœ¬ï¼Œåœ¨æ ‡ç‚¹ç¬¦å·å¤„æ·»åŠ åœé¡¿
        segments = []
        current_segment = ""

        for char in text:
            current_segment += char

            # æ ¹æ®æ ‡ç‚¹ç¬¦å·å†³å®šåœé¡¿
            if char in ['ã€‚', '.', '!', '?', 'ï¼', 'ï¼Ÿ']:
                segments.append((current_segment, 0.8)) # å¥å·åœé¡¿0.8s
                current_segment = ""
            elif char in ['ï¼Œ', ',', 'ã€', ';', 'ï¼›']:
                segments.append((current_segment, 0.4)) # é€—å·åœé¡¿0.4s
                current_segment = ""
    
        # å¦‚æœæœ‰å‰©ä½™æ–‡æœ¬ï¼Œæ·»åŠ åˆ°æœ€åä¸€ä¸ªç‰‡æ®µ
        if current_segment:
            segments.append((current_segment, 0.2))
        
        # åº”ç”¨æƒ…æ„ŸçŠ¶æ€è°ƒæ•´ï¼ˆå­˜åœ¨çš„åœºåˆï¼‰
        if emotional_state == "excited":
            # æ¿€åŠ¨çŠ¶æ€ï¼Œç¼©çŸ­åœé¡¿
            segments = [(s, p * 0.7) for s, p in segments]
        elif emotional_state == "sad":
            # æ‚²ä¼¤çŠ¶æ€ï¼Œå»¶é•¿åœé¡¿
            segments = [(s, p * 1.3) for s, p in segments]
        
        # å¼€å§‹è¯­éŸ³åˆæˆï¼Œæ·»åŠ åœé¡¿
        for segment, pause in segments:
            if segment.strip():  # ç¡®ä¿æ®µè½ä¸ä¸ºç©º
                try:
                    # ä½¿ç”¨ğŸ¸TTSç”ŸæˆéŸ³é¢‘
                    wav = self.tts_engine.tts(text=segment, speaker=None, language=None)
                    
                    # æ’­æ”¾éŸ³é¢‘
                    import soundfile as sf
                    import sounddevice as sd
                    
                    # ä¿å­˜éŸ³é¢‘åˆ°ä¸´æ—¶æ–‡ä»¶
                    sf.write(self.output_path, wav, self.tts_engine.synthesizer.output_sample_rate)
                    
                    # æ’­æ”¾éŸ³é¢‘
                    data, fs = sf.read(self.output_path)
                    sd.play(data, fs)
                    sd.wait()  # ç­‰å¾…éŸ³é¢‘æ’­æ”¾å®Œæˆ
                    
                except Exception as e:
                    print(f"è¯­éŸ³åˆæˆå¤±è´¥: {e}")
                
                # ç­‰å¾…æŒ‡å®šçš„åœé¡¿æ—¶é—´
                time.sleep(pause)


class SimpleVAD:
    """è¯­éŸ³æ´»åŠ¨æ£€æµ‹å™¨"""

    def __init__(self, rate=16000, chunk_size=1024, threshold=0.01, min_silence_duration=1.0, min_speech_duration=0.5):
        """åˆå§‹åŒ–VADå¯¹è±¡

        Args:
            rate (int): é‡‡æ ·ç‡
            chunk_size (int): æ¯æ¬¡å¤„ç†çš„éŸ³é¢‘å¤§å°
            threshold (float): èƒ½é‡é˜ˆå€¼ï¼Œç”¨äºæ£€æµ‹è¯­éŸ³
            min_silence_duration (float): æœ€å°é™éŸ³æŒç»­æ—¶é—´ï¼Œä½äºæ­¤å€¼ä¸è®¤ä¸ºè¯´è¯ç»“æŸ
            min_speech_duration (float): æœ€å°è¯­éŸ³æŒç»­æ—¶é—´ï¼Œä½äºæ­¤å€¼è®¤ä¸ºæ˜¯å™ªéŸ³
        """
        self.rate = rate
        self.chunk_size = chunk_size
        self.threshold = threshold
        self.min_silence_frames = int(min_silence_duration * rate / chunk_size)
        self.min_speech_frames = int(min_speech_duration * rate / chunk_size)
        self.audio_queue = queue.Queue()
        self.is_speaking = False
        self.silence_counter = 0
        self.speech_counter = 0
        self.audio_history = deque(maxlen=100) # å­˜å‚¨æœ€è¿‘çš„éŸ³é¢‘æ•°æ®
        self.p = None
        self.stream = None
    
    def _calculate_energy(self, audio_chunk):
        """è®¡ç®—éŸ³é¢‘æ•°æ®çš„èƒ½é‡"""
        return np.mean(np.abs(audio_chunk))

    def start_listening(self):
        """å¼€å§‹ç›‘å¬éŸ³é¢‘æ•°æ®"""
        # åˆ›å»ºéŸ³é¢‘è¾“å…¥æµ
        self.p = pyaudio.PyAudio()
        self.stream = self.p.open(
            format=pyaudio.paFloat32,
            channels=1,
            rate=self.rate,
            input=True,
            frames_per_buffer=self.chunk_size,
            stream_callback=self._audio_callback
        )
        
        # å¼€å§‹ç›‘å¬
        print("å¼€å§‹ç›‘å¬éŸ³é¢‘æ•°æ®...")
        self.stream.start_stream()
    
    def _audio_callback(self, in_data, frame_count, time_info, status):
        """éŸ³é¢‘è¾“å…¥å›è°ƒå‡½æ•°"""
        audio_data = np.frombuffer(in_data, dtype=np.float32)
        self.audio_queue.put(audio_data)
        self.audio_history.append(audio_data)
        return (in_data, pyaudio.paContinue)

    def process_audio(self):
        """å¤„ç†éŸ³é¢‘é˜Ÿåˆ—ä¸­çš„æ•°æ®ï¼Œæ£€æµ‹è¯­éŸ³æ´»åŠ¨"""
        while True:
            if not self.audio_queue.empty():
                audio_chunk = self.audio_queue.get()
                energy = self._calculate_energy(audio_chunk)

                # å¦‚æœèƒ½é‡è¶…è¿‡é˜ˆå€¼ï¼Œè®¤ä¸ºæ˜¯è¯´è¯
                if energy > self.threshold:
                    self.speech_counter += 1
                    self.silence_counter = 0

                    if self.speech_counter >= self.min_speech_frames and not self.is_speaking:
                        self.is_speaking = True
                        self.on_speech_start()
                else:
                    # æ£€æµ‹åˆ°é™éŸ³
                    self.silence_counter += 1

                    if self.silence_counter >= self.min_silence_frames and self.is_speaking:
                        self.is_speaking = False
                        self.speech_counter = 0
                        self.on_speech_end()
            else:
                time.sleep(0.01)
    
    def on_speech_start(self):
        """å½“æ£€æµ‹åˆ°è¯­éŸ³å¼€å§‹æ—¶è°ƒç”¨"""
        print("æ£€æµ‹åˆ°è¯­éŸ³å¼€å§‹")
    
    def on_speech_end(self):
        """å½“æ£€æµ‹åˆ°è¯­éŸ³ç»“æŸæ—¶è°ƒç”¨"""
        print("æ£€æµ‹åˆ°è¯­éŸ³ç»“æŸ")
    
    def get_recorded_audio(self):
        """è·å–æœ€è¿‘å½•åˆ¶çš„éŸ³é¢‘æ•°æ®"""
        recent_audio = list(self.audio_history)[-50:]  # å–æœ€è¿‘çš„50ä¸ªéŸ³é¢‘æ•°æ®
        if recent_audio:
            return np.concatenate(recent_audio)
        return np.array([])
    
    def stop_listening(self):
        """åœæ­¢ç›‘å¬éŸ³é¢‘æ•°æ®"""
        if hasattr(self, 'stream') and self.stream.is_active():
            self.stream.stop_stream()
            self.stream.close()
        if hasattr(self, 'p'):
            self.p.terminate()
        print("åœæ­¢ç›‘å¬éŸ³é¢‘æ•°æ®")


class SpeakerRecognition:
    """è¯´è¯äººè¯†åˆ«æ¨¡å—"""
    
    def __init__(self, feature_dim=128):
        """åˆå§‹åŒ–è¯´è¯äººè¯†åˆ«æ¨¡å—

        Args:
            feature_dim (int): ç‰¹å¾å‘é‡ç»´åº¦
        """
        self.speaker_db = {}  # å­˜å‚¨å·²çŸ¥è¯´è¯äººçš„ç‰¹å¾å‘é‡
        self.unknown_counter = 0  # æœªè¯†åˆ«è¯´è¯äººè®¡æ•°
        self.feature_dim = feature_dim
        
    def extract_features(self, audio_data):
        """ä»éŸ³é¢‘æå–è¯´è¯äººç‰¹å¾å‘é‡"""
        # å®é™…åº”ç”¨ä¸­ä½¿ç”¨å¦‚MFCC, i-vectoræˆ–x-vectorç­‰ç‰¹å¾
        # è¿™é‡Œä½¿ç”¨ç®€åŒ–å®ç°
        import numpy as np
        
        try:
            from sklearn.preprocessing import normalize
            # ç®€åŒ–ç‰ˆç‰¹å¾æå– (å®é™…åº”ç”¨åº”ä½¿ç”¨ä¸“ä¸šæ¨¡å‹)
            features = np.random.rand(self.feature_dim)
            return normalize(features.reshape(1, -1))[0]
        except ImportError:
            # å¦‚æœæ²¡æœ‰sklearnï¼Œä½¿ç”¨ç®€å•å½’ä¸€åŒ–
            features = np.random.rand(self.feature_dim)
            return features / np.linalg.norm(features)
    
    def identify_speaker(self, audio_data, threshold=0.85):
        """è¯†åˆ«è¯´è¯äººï¼Œè¿”å›è¯´è¯äººID"""
        features = self.extract_features(audio_data)
        
        best_match = None
        best_score = 0
        
        # è®¡ç®—ä¸å·²çŸ¥è¯´è¯äººçš„ç›¸ä¼¼åº¦
        for speaker_id, stored_features in self.speaker_db.items():
            similarity = np.dot(features, stored_features)
            if similarity > best_score:
                best_score = similarity
                best_match = speaker_id
        
        # å¦‚æœç›¸ä¼¼åº¦è¶…è¿‡é˜ˆå€¼ï¼Œè®¤ä¸ºæ˜¯å·²çŸ¥è¯´è¯äºº
        if best_match and best_score > threshold:
            return best_match
        
        # å¦åˆ™åˆ›å»ºæ–°è¯´è¯äººID
        new_id = f"speaker_{self.unknown_counter}"
        self.unknown_counter += 1
        self.speaker_db[new_id] = features
        return new_id
    
    def update_speaker_model(self, speaker_id, audio_data):
        """æ›´æ–°è¯´è¯äººæ¨¡å‹"""
        if speaker_id not in self.speaker_db:
            self.speaker_db[speaker_id] = self.extract_features(audio_data)
        else:
            # æ›´æ–°ç°æœ‰æ¨¡å‹ (ç®€å•å¹³å‡ï¼Œå®é™…åº”æœ‰æ›´å¤æ‚çš„æ›´æ–°æ–¹æ³•)
            new_features = self.extract_features(audio_data)
            self.speaker_db[speaker_id] = 0.7 * self.speaker_db[speaker_id] + 0.3 * new_features

# æµ‹è¯•éŸ³é¢‘æœåŠ¡
if __name__ == "__main__":
    # æµ‹è¯•éŸ³é¢‘æœåŠ¡
    audio_service = AudioService()
    audio_service.start_listening()
    
    # æ¨¡æ‹ŸéŸ³é¢‘è¾“å…¥
    import sounddevice as sd
    import time
    fs = 16000
    duration = 5.0  # 5ç§’
    audio_data = sd.rec(int(duration * fs), samplerate=fs, channels=1, dtype='float32')
    sd.wait()
    
    # æ£€æµ‹è¯´è¯äºº
    speaker_id = audio_service.identify_speaker(audio_data)
    print(f"è¯†åˆ«åˆ°è¯´è¯äºº: {speaker_id}")
    
    # # æ›´æ–°è¯´è¯äººæ¨¡å‹
    # audio_service.update_speaker_model(speaker_id, audio_data)
    
    # # è¯­éŸ³åˆæˆ
    # text = "ä½ å¥½ï¼Œæˆ‘æ˜¯AIåŠ©æ‰‹"
    # audio_service.speak(text)
    
    # # å¸¦åœé¡¿çš„è¯­éŸ³åˆæˆ
    # text = "ä½ å¥½ï¼Œæˆ‘æ˜¯AIåŠ©æ‰‹ã€‚æˆ‘å¯ä»¥å›ç­”ä½ çš„é—®é¢˜ï¼Œè¿˜å¯ä»¥è®²ç¬‘è¯ã€‚"
    # audio_service.speak_with_pauses(text)
    
    # # åœæ­¢ç›‘å¬
    # audio_service.stop_listening()